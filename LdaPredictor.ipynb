{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from operator import itemgetter\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sych_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "import nltk; nltk.download('stopwords')\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.append('привет')\n",
    "russian_stopwords.append('спасибо')\n",
    "russian_stopwords.append('пожалуйста')\n",
    "russian_stopwords.append('пока')\n",
    "russian_stopwords.append('добрый')\n",
    "russian_stopwords.append('день')\n",
    "russian_stopwords.append('nan')\n",
    "russian_stopwords.append('end')\n",
    "russian_stopwords.append('утро')\n",
    "russian_stopwords.append('ок')\n",
    "russian_stopwords.append('здравствуйте')\n",
    "russian_stopwords.append('мочь')\n",
    "russian_stopwords.append('не')\n",
    "russian_stopwords.append('работать')\n",
    "russian_stopwords.append('сей')\n",
    "russian_stopwords.append('пора')\n",
    "russian_stopwords.append('очень')\n",
    "russian_stopwords.append('проблема')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LdaPredictor():\n",
    "    \n",
    "    def __init__(self, lda_path, dict_path, bigram_path, trigram_path):\n",
    "        \"\"\"\n",
    "        lda_path - путь к lda модели\n",
    "        dict_path - путь к словарю \n",
    "        bigram_path - путь к биграммам\n",
    "        trigram_path - путь к триграммам\n",
    "        \n",
    "        param: lda_path str\n",
    "        param: dict_path str\n",
    "        param: bigram_path str\n",
    "        param: trigram_path str\n",
    "        \"\"\"\n",
    "        self.dictionary = corpora.Dictionary.load(dict_path)\n",
    "        self.lda = LdaMulticore.load(lda_path)\n",
    "        self.bigram_path = bigram_path\n",
    "        self.trigram_path = trigram_path\n",
    "        \n",
    "    def to_lemmatize2(self, text):\n",
    "        all_word_str = \" \".join(text)\n",
    "        all_word_list = all_word_str.split()\n",
    "        all_unique_word = Series(all_word_list).unique()\n",
    "        lemmatized_word_dict = {}\n",
    "        lemmatizer = MorphAnalyzer()\n",
    "        for word in all_unique_word:\n",
    "            lemmatized_word_dict[word] = lemmatizer.normal_forms(word)[0]\n",
    "        text = ' '.join([lemmatized_word_dict[word] for word in text.split()])\n",
    "        return text, all_unique_word\n",
    "        \n",
    "    def clean(self, text):\n",
    "        #text = regex.sub('', text)\n",
    "        deleted_symols = '[\\\\\\\\\\'[\\]!\"$%&()*+,-./:;<=>?@^_`{|}~«»\\n]'\n",
    "        text = re.sub(deleted_symols, ' ', text)\n",
    "        \n",
    "        text = ' '.join([elem for elem in str(text).split(' ') if elem.isdigit() == False])\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        text = [token for token in text.split() if token not in russian_stopwords]\n",
    "        #text = [stemmer.stem(token) for token in text]\n",
    "        text = [token for token in text if token]\n",
    "        text, _ = self.to_lemmatize2(text)\n",
    "        \n",
    "        return ' '.join(text)\n",
    "    \n",
    "    def bigram(self, text):\n",
    "        bigram = Phrases.load(self.bigram_path)\n",
    "        trigram = Phrases.load(self.trigram_path)\n",
    "        text_clean = text\n",
    "        for idx in range(len(text_clean)):\n",
    "            for token in bigram[text_clean[idx]]:\n",
    "                if '_' in token:\n",
    "                    text_clean[idx].append(token)\n",
    "            for token in trigram[text_clean[idx]]:\n",
    "                if '_' in token:\n",
    "                    text_clean[idx].append(token)\n",
    "        return text_clean\n",
    "    \n",
    "    def predict(self, text):\n",
    "        clean_text = self.clean(text).split()\n",
    "        bigram = self.bigram([clean_text])\n",
    "        new_review_bow = self.dictionary.doc2bow(bigram[0])\n",
    "        new_review_lda = self.lda[new_review_bow]\n",
    "        return sorted(new_review_lda, reverse=True, key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path = \"./model/best_model.lda\"\n",
    "dict_path = \"./model/dictionary.dict\"\n",
    "bigram_path = \"./model/bigram.phs\"\n",
    "trigram_path = \"./model/trigram.phs\"\n",
    "lda = LdaPredictor(lda_path, dict_path,  bigram_path, trigram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-e3e431f2a13b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"При открытии формирования приходной накладной  всплывает данная ошибка. На всех компах С9  не можем поставить палеты на приход\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-bf5759db887c>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mclean_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mnew_review_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-bf5759db887c>\u001b[0m in \u001b[0;36mclean\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m#text = [stemmer.stem(token) for token in text]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_lemmatize2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-bf5759db887c>\u001b[0m in \u001b[0;36mto_lemmatize2\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_unique_word\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mlemmatized_word_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatized_word_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_unique_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "text = \"При открытии формирования приходной накладной  всплывает данная ошибка. На всех компах С9  не можем поставить палеты на приход\"\n",
    "predict = lda.predict(text)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
